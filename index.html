<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8"/>
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"/>
        <meta name="description" content=""/>
        <meta name="author" content=""/>
        <title>Zeping Ren</title>
        <link rel="icon" type="image/x-icon" href="assets/apple.ico"/>
        <!-- Font Awesome icons (free version)-->
        <!-- <link rel="stylesheet" href="./css/font-awesome.min.css"/> -->
        <script src="https://use.fontawesome.com/releases/v5.15.4/js/all.js" crossorigin="anonymous"></script>
        <!-- Google fonts-->
        <link href="https://fonts.googleapis.com/css?family=Saira+Extra+Condensed:500,700" rel="stylesheet" type="text/css"/>
        <link href="https://fonts.googleapis.com/css?family=Muli:400,400i,800,800i" rel="stylesheet" type="text/css"/>
        <!-- Core theme CSS (includes Bootstrap)-->
        <link href="css/styles.css" rel="stylesheet"/>
    </head>
    <body id="page-top">
        <!-- Navigation-->
        <nav class="navbar navbar-expand-lg navbar-dark bg-primary fixed-top" id="sideNav">
            <a class="navbar-brand js-scroll-trigger" href="#page-top">
                <span class="d-none d-lg-block">
                    <img class="img-fluid img-profile rounded-circle mx-auto mb-2" src="assets/me.jpg" alt="..."/>
                    <h3 class="mb-0">Zeping Ren</h3>
                </span>
            </a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarResponsive">
                <ul class="navbar-nav">
                    <li class="nav-item">
                        <a class="nav-link js-scroll-trigger" href="#about">
                            <i class="fa fa-home" aria-hidden="true"></i>
                            About
                        </a>
                    </li>
                    <!-- fa fa-ellipsis-h-->
                    <li class="nav-item">
                        <a class="nav-link js-scroll-trigger" href="#research">
                            <i class="fa fa-flask" aria-hidden="true"></i>
                            Research
                        </a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link js-scroll-trigger" href="#more">
                            <i class="fa fa-graduation-cap" aria-hidden="true"></i>
                            More
                        </a>
                    </li>
                </ul>
            </div>
        </nav>
        <!-- Page Content-->
        <div class="container-fluid p-0">
            <!-- About-->
            <section class="resume-section" id="about">
                <div class="resume-section-content">
                    <!--Only show the image on mobile-->
                    <span class="d-lg-none">
                        <img class="img-fluid img-profile rounded-circle mx-auto mb-2" width=150px src="assets/me.jpg" alt="..."/>
                    </span>
                    <h2>Zeping Ren</h2>
                    <div class="subheading mb-3">
                        <span style="text-decoration:none; color:inherit;">
                            Master Student, Tsinghua University
                        </span>
                        <br>
                        <span style="text-decoration:none; color:inherit;">
                            Hometown: Chongqing,China
                        </span>
                        <br>
                        <span style="font-size:1.1rem">
                            Email:rzp22@mails.tsinghua.edu.com
                        </span>
                    </div>
                    <!-- <div class="social-icons">
                        <a class="social-icon" href="mailto:gul.varol@enpc.fr">
                            <i class="fa fa-envelope"></i>
                        </a>
                        <a class="social-icon" href="https://scholar.google.fr/citations?user=ceSzF9YAAAAJ&hl=en" class="text-info" title="Google Scholar">
                            <i class="ai ai-google-scholar"></i>
                        </a>
                        <a class="social-icon" href="https://github.com/gulvarol">
                            <i class="fa fa-github"></i>
                        </a>
                        <a class="social-icon" href="https://www.linkedin.com/in/gulvarol/">
                            <i class="fa fa-linkedin"></i>
                        </a>
                        <a class="social-icon" href="https://twitter.com/gulvarol">
                            <i class="fa fa-twitter"></i>
                        </a>
                    </div> -->
                    <p>
                        <br>
                        <b>Bio:</b>
                        I am a master student studied in Tsinghua Shenzhen International Graduate School,
                        advised by <a href="https://www.sigs.tsinghua.edu.cn/lx/main.htm">Prof.Xiu Li</a>.
                        I got my undergraduate degree in Department of Automation, Tsinghua University,
                        under the supervision of <a href="http://liuyebin.com/">Prof.Yebin Liu</a>.
                        My research is focus on 3D pose estimation and motion generation.
                    </p>
                </div>
            </section>
            
        </div>
        <hr class="m-0"/>
        <!--Research-->
        <section class="resume-section" id="research">
            <div class="resume-section-content">
                <h3 class="mb-3">Research</h3>
                <!-- crossdiff -->
                <div class="row">
                    <div class="col-xs-10 col-sm-4 col-md-4">
                        <a href="https://wonderno.github.io/CrossDiff-webpage/">
                            <img class="img-thumbnail mb-3" src="./assets/crossdiff.jpg" alt="Realistic Human Motion Generation with Cross-Diffusion Models">
                        </a>
                    </div>
                    <div class="col-xs-12 col-sm-8 col-md-8">
                        <strong>Realistic Human Motion Generation with Cross-Diffusion Models</strong>
                        <br>
                        <u>Zeping Ren</u>, Shaoli Huang, Xiu Li1
                        <br>
                        <em>arXiv </em>
                        2023.<br>
                        <!-- <a href="https://li-ronghui.github.io/finedance"><button type="button" class="btn btn-primary btn-sm">pdf</button></a> -->
                        <a href="https://wonderno.github.io/CrossDiff-webpage/">
                            <button type="button" class="btn btn-primary btn-sm">project page</button></a>
                        <!-- <button type="button" class="btn btn-primary btn-sm" data-bs-toggle="collapse" data-bs-target="#bibtex3">bibtex</button>
                        <div id="bibtex3" class="collapse">
                            <pre>
                                <tt>...</tt>
                            </pre>
                        </div> -->
                        <button type="button" class="btn btn-primary btn-sm" data-bs-toggle="collapse" data-bs-target="#abstract3">abstract</button>
                        <div id="abstract3" class="collapse">
                            <p class="bg-light">
                                We introduce the Cross Human Motion Diffusion Model (CrossDiff), a novel approach for generating high-quality human motion based on textual descriptions.
                                Our method integrates 3D and 2D information using a shared transformer network within the training of the diffusion model, unifying motion noise into a single feature space. 
                                This enables cross-decoding of features into both 3D and 2D motion representations, regardless of their original dimension. 
                                The primary advantage of CrossDiff is its cross-diffusion mechanism, which allows the model to reverse either 2D or 3D noise into clean motion during training. 
                                This capability leverages the complementary information in both motion representations, capturing intricate human movement details often missed by models relying solely on 3D information. 
                                Consequently, CrossDiff effectively combines the strengths of both representations to generate more realistic motion sequences. 
                                In our experiments, our model demonstrates competitive state-of-the-art performance on text-to-motion benchmarks. 
                                Moreover, our method consistently provides enhanced motion generation quality, capturing complex full-body movement intricacies. 
                                Additionally, our approach accommodates using 2D motion data without 3D motion ground truth during training to generate 3D motion, highlighting its potential for broader applications and efficient use of available data resources.
                  
                            </p>
                        </div>
                        <!-- <a href="https://github.com/lucas-ventura/covr/">
                            <button type="button" class="btn btn-primary btn-sm">code </button>
                        </a> -->
                        <div style="height:30px;"></div>
                    </div>
                </div>
                <!-- end of crossdiff -->

                <!-- finedance -->
                <div class="row">
                    <div class="col-xs-10 col-sm-4 col-md-4">
                        <a href="https://li-ronghui.github.io/finedance">
                            <img class="img-thumbnail mb-3" src="./assets/finedance.png" alt="FineDance: A Fine-grained Choreography Dataset for 3D Full Body Dance Generation">
                        </a>
                    </div>
                    <div class="col-xs-12 col-sm-8 col-md-8">
                        <strong>FineDance: A Fine-grained Choreography Dataset for 3D Full Body Dance Generation</strong>
                        <br>
                        Ronghui Li*, Junfan Zhao*, Yachao Zhang, Mingyang Su, <u>Zeping Ren</u>, Han Zhang, Yansong Tang, Xiu Li
                        <br>
                        <em>ICCV </em>
                        2023.<br>
                        <a href="https://arxiv.org/abs/2212.03741v4">
                            <button type="button" class="btn btn-primary btn-sm"> pdf </button></a>
                        <a href="https://li-ronghui.github.io/finedance">
                            <button type="button" class="btn btn-primary btn-sm"> project page </button></a>
                        <button type="button" class="btn btn-primary btn-sm" data-bs-toggle="collapse" data-bs-target="#bibtex2">bibtex</button>
                        <div id="bibtex2" class="collapse">
                            <pre>
<tt>@InProceedings{Li_2023_ICCV,
    author    = {Li, Ronghui and Zhao, Junfan and Zhang, Yachao and Su, Mingyang and Ren, Zeping and Zhang, Han and Tang, Yansong and Li, Xiu},
    title     = {FineDance: A Fine-grained Choreography Dataset for 3D Full Body Dance Generation},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    month     = {October},
    year      = {2023},
    pages     = {10234-10243}
}</tt>
                            </pre>
                        </div>
                        <button type="button" class="btn btn-primary btn-sm" data-bs-toggle="collapse" data-bs-target="#abstract2">abstract</button>
                        <div id="abstract2" class="collapse">
                            <p class="bg-light">
                                Generating full-body and multi-genre dance sequences from given music is a challenging task, due to the limitations of existing datasets and the inherent complexity of the fine-grained hand motion and dance genres. To address these problems, we propose FineDance, which contains 14.6 hours of music-dance paired data, with fine-grained hand motions, fine-grained genres (22 dance genres), and accurate posture. To the best of our knowledge, FineDance is the largest music-dance paired dataset with the most dance genres. Additionally, to address monotonous and unnatural hand movements existing in previous methods, we propose a full-body dance generation network, which utilizes the diverse generation capabilities of the diffusion model to solve monotonous problems, and use expert nets to solve unreal problems. To further enhance the genrematching and long-term stability of generated dances, we propose a Genre&Coherent aware Retrieval Module. Besides, we propose a new metric named Genre Matching Score to measure the genre matching between dance and music. Quantitative and qualitative experiments demonstrate the quality of FineDance, and the state-of-the-art performance of FineNet.
                  
                            </p>
                        </div>
                        <a href="https://github.com/li-ronghui/FineDance">
                            <button type="button" class="btn btn-primary btn-sm">code </button>
                        </a>
                        <div style="height:30px;"></div>
                    </div>
                </div>
                <!-- end of finedance -->

                <!-- realtime_capture -->
                <div class="row">
                    <div class="col-xs-10 col-sm-4 col-md-4">
                        <a href="http://www.liuyebin.com/realtime_cap_project/realtotalcap/realtotalcap.html">
                            <img class="img-thumbnail mb-3" src="./assets/realtotalcap.jpg" alt="Real-time Sparse-view Multi-person Total Motion Capture">
                        </a>
                    </div>
                    <div class="col-xs-12 col-sm-8 col-md-8">
                        <strong>Real-time Sparse-view Multi-person Total Motion Capture</strong>
                        <br>
                        Yuxiang Zhang, <u>Zeping Ren</u>, Liang An, Hongwen Zhang, Tao Yu, Yebin Liu
                        <br>
                        <!-- <em>arXiv </em> -->
                        2022.<br>
                        <a href="http://www.liuyebin.com/realtime_cap_project/realtotalcap/realtotalcap.html">
                            <button type="button" class="btn btn-primary btn-sm"> project page </button></a>
                        <button type="button" class="btn btn-primary btn-sm" data-bs-toggle="collapse" data-bs-target="#abstract1">abstract</button>
                        <div id="abstract1" class="collapse">
                            <p class="bg-light">
                                Real-time multi-person total motion capture is one of the most challenging tasks for human motion capture. The multi-view configuration reduces occlusions and depth ambiguity yet further complicates the problem by introducing cross-view association into consideration. In this paper, we contribute the first real-time multi-person total motion capture system under sparse views. To enable full body cross-view association in real-time, we propose a highly efficient association algorithm, named Clique Unfolding, by reformulating the widely used fast unfolding algorithm for community detection. Moreover, an adaptive motion prior based on human motion prediction is proposed to improve the SMPL-X fitting performance in the final step. Benefiting from the proposed association and fitting methods, our system achieves robust, efficient and accurate multi-person total motion capture results. Experiments and results demonstrate the efficiency and effectiveness of the proposed method.
                            </p>
                        </div>
                        <div style="height:30px;"></div>
                    </div>
                </div>
                <!-- end of realtime_capture -->
                
            </div>
        </section>
        <!-- end of Research -->

        <hr class="m-0"/>
        <!-- More-->
        <section class="resume-section" id="more">
            <div class="resume-section-content">
                <h3 class="mb-3">More</h3>
                    <p>
                        I'm not a magician, but I never stop creating miracles.
                    </p>
            </div>
        </section>
        <!-- end of More -->
        
       
        <!-- Bootstrap core JS-->
        <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>
        <!-- Core theme JS-->
        <script src="js/scripts.js"></script>
    </body>
</html>
